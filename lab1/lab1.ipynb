{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "## Part 1 Naive Bayes\n",
    "### Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use train data: d:\\Windows\\Documents\\Code\\conda\\machine-learning-lab\\lab1\\train.csv\n",
      "Use test  data: d:\\Windows\\Documents\\Code\\conda\\machine-learning-lab\\lab1\\train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "PATH_ROOT = os.getcwd()\n",
    "PATH_TRAIN = os.path.join(PATH_ROOT, 'train.csv')\n",
    "PATH_TEST = os.path.join(PATH_ROOT, 'test.csv')\n",
    "\n",
    "print(\"Use train data:\", PATH_TRAIN)\n",
    "print(\"Use test  data:\", PATH_TRAIN)\n",
    "\n",
    "train_data = pd.read_csv(PATH_TRAIN)\n",
    "test_data = pd.read_csv(PATH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoginize categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Restaurants' 'Nightlife' 'Shopping']\n"
     ]
    }
   ],
   "source": [
    "categories = train_data['category'].unique()\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that there are three categories in the data set, they are: `['Restaurants' 'Nightlife' 'Shopping']`.\n",
    "\n",
    "Then, we transform this column into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: category, dtype: int32\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: category, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "categories_type = CategoricalDtype(categories = categories)\n",
    "train_data['category'] = train_data['category'].astype(categories_type).cat.codes.astype('long')\n",
    "test_data['category'] = test_data['category'].astype(categories_type).cat.codes.astype('long')\n",
    "print(train_data['category'].head(), \"\\n\", test_data['category'].head(), sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes process\n",
    "We start our Naive Bayes process.\n",
    "\n",
    "Firstly, we should build training and testing dataframe variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data['review']\n",
    "train_y = train_data['category']\n",
    "\n",
    "test_x = test_data['review']\n",
    "test_y = test_data['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to build a vector of word counts. Use built in class `CountVectorizer`. And transform original data into vector. For test variables, we use the same methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer()\n",
    "train_x = vector.fit_transform(train_x).toarray()\n",
    "test_x = vector.transform(test_x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other attributes like `mean_checkin_time` also need to be considered. We merge these data into training and testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.append(train_data[['latitude', 'longitude', 'mean_checkin_time']], train_x, axis=1)\n",
    "test_x = np.append(test_data[['latitude', 'longitude', 'mean_checkin_time']], test_x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already built our training and testing data set.\n",
    "\n",
    "Lastly, we could classify texts by using Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many correct prediction we have made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532467532467533"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "About $75.3%$ of the entire data set has been classified correctly. However, this is the simplest model. We could build a more complex model to classify these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Data preprocessing\n",
    "Note that the `CountVectorizer` just split text into single words simply, we could try adjusting the parameter of how it split texts.\n",
    "\n",
    "We try set parameter `ngram_range` to `1` and `2`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.797979797979798"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector2 = CountVectorizer(ngram_range = (1, 2))\n",
    "train_x2 = train_data['review']\n",
    "train_y2 = train_data['category']\n",
    "train_x2 = vector2.fit_transform(train_x2).toarray()\n",
    "train_x2 = np.append(train_data[['latitude', 'longitude', 'mean_checkin_time']], train_x2, axis=1)\n",
    "nb2 = GaussianNB()\n",
    "nb2.fit(train_x2, train_y2)\n",
    "\n",
    "test_x2 = test_data['review']\n",
    "test_y2 = test_data['category']\n",
    "test_x2 = vector2.transform(test_x2).toarray()\n",
    "test_x2 = np.append(test_data[['latitude', 'longitude', 'mean_checkin_time']], test_x2, axis=1)\n",
    "nb2.score(test_x2,test_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "After a long processing period, we got a $79.8%$ correct rate. It did an improvement. However, we should consider **wether to drop** this optimization because it cost too much time while brought not very large improvement.\n",
    "\n",
    "Because its complexity, we must comment these code in order to prevent wasting time on this optional model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Model improving\n",
    "Maybe we should use other Naive Bayes model after using `GaussianNB`.\n",
    "\n",
    "### `BernoulliNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8253968253968254"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector3 = CountVectorizer()\n",
    "train_x3 = train_data['review']\n",
    "train_y3 = train_data['category']\n",
    "train_x3 = vector3.fit_transform(train_x3).toarray()\n",
    "train_x3 = np.append(train_data[['latitude', 'longitude', 'mean_checkin_time']], train_x3, axis=1)\n",
    "nb3 = BernoulliNB()\n",
    "nb3.fit(train_x3, train_y3)\n",
    "\n",
    "test_x3 = test_data['review']\n",
    "test_y3 = test_data['category']\n",
    "test_x3 = vector3.transform(test_x3).toarray()\n",
    "test_x3 = np.append(test_data[['latitude', 'longitude', 'mean_checkin_time']], test_x3, axis=1)\n",
    "nb3.score(test_x3,test_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MultinomialNB`.\n",
    "\n",
    "Because `MultinomialNB` cannot accept negative values, we tries to remove `latitude` and `longitude` attributes from our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8701298701298701"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector4 = CountVectorizer()\n",
    "train_x4 = train_data['review']\n",
    "train_y4 = train_data['category']\n",
    "train_x4 = vector4.fit_transform(train_x4).toarray()\n",
    "train_x4 = np.append(train_data[['mean_checkin_time']], train_x4, axis=1)\n",
    "nb4 = MultinomialNB()\n",
    "nb4.fit(train_x4, train_y4)\n",
    "\n",
    "test_x4 = test_data['review']\n",
    "test_y4 = test_data['category']\n",
    "test_x4 = vector4.transform(test_x4).toarray()\n",
    "test_x4 = np.append(test_data[['mean_checkin_time']], test_x4, axis=1)\n",
    "nb4.score(test_x4,test_y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The combination\n",
    "\n",
    "What if we combine the best 2 methods in first and second try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8051948051948052"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector5 = CountVectorizer(ngram_range = (1, 2))\n",
    "train_x5 = train_data['review']\n",
    "train_y5 = train_data['category']\n",
    "train_x5 = vector5.fit_transform(train_x5).toarray()\n",
    "train_x5 = np.append(train_data[['mean_checkin_time']], train_x5, axis=1)\n",
    "nb5 = MultinomialNB()\n",
    "nb5.fit(train_x5, train_y5)\n",
    "\n",
    "test_x5 = test_data['review']\n",
    "test_y5 = test_data['category']\n",
    "test_x5 = vector5.transform(test_x5).toarray()\n",
    "test_x5 = np.append(test_data[['mean_checkin_time']], test_x5, axis=1)\n",
    "nb5.score(test_x5,test_y5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unluckily, the combination of 2 good methods led to a worse result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "After verification, use `MultinomialNB` is a better model. The $87.0%$ of accuracy is far better than other models.\n",
    "\n",
    "Some further tries could be done. For example, we can make `latitude` and `longitude` positive. However, that didn't changed the results. So we can infer that `latitude` and `longitude` have low association with results."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6adee2de74d267f267a666c0e4ee6c85309d51fbc20ddd8fabddd56db54bc85"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
