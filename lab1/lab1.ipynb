{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "## Step 1 Naive Bayes\n",
    "### Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use train data: /workspaces/machine-learning-lab/lab1/train.csv\n",
      "Use test  data: /workspaces/machine-learning-lab/lab1/train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "PATH_ROOT = os.getcwd()\n",
    "PATH_TRAIN = os.path.join(PATH_ROOT, 'train.csv')\n",
    "PATH_TEST = os.path.join(PATH_ROOT, 'test.csv')\n",
    "\n",
    "print(\"Use train data:\", PATH_TRAIN)\n",
    "print(\"Use test  data:\", PATH_TRAIN)\n",
    "\n",
    "train_data = pd.read_csv(PATH_TRAIN)\n",
    "test_data = pd.read_csv(PATH_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoginize categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Restaurants' 'Nightlife' 'Shopping']\n"
     ]
    }
   ],
   "source": [
    "categories = train_data['category'].unique()\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that there are three categories in the data set, they are: `['Restaurants' 'Nightlife' 'Shopping']`.\n",
    "\n",
    "Then, we transform this column into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: category, dtype: int64\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "categories_type = CategoricalDtype(categories = categories)\n",
    "train_data['category'] = train_data['category'].astype(categories_type).cat.codes.astype('long')\n",
    "test_data['category'] = test_data['category'].astype(categories_type).cat.codes.astype('long')\n",
    "print(train_data['category'].head(), \"\\n\", test_data['category'].head(), sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes process\n",
    "We start our Naive Bayes process.\n",
    "\n",
    "Firstly, we should build training and testing dataframe variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_data['review']\n",
    "train_y = train_data['category']\n",
    "\n",
    "test_x = test_data['review']\n",
    "test_y = test_data['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to build a vector of word counts. Use built in class `CountVectorizer`. And transform original data into vector. For test variables, we use the same methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer()\n",
    "train_x = vector.fit_transform(train_x).toarray()\n",
    "test_x = vector.transform(test_x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other attributes like `mean_checkin_time` also need to be considered. We merge these data into training and testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.append(train_data[['latitude', 'longitude', 'mean_checkin_time']], train_x, axis=1)\n",
    "test_x = np.append(test_data[['latitude', 'longitude', 'mean_checkin_time']], test_x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already built our training and testing data set.\n",
    "\n",
    "Lastly, we could classify texts by using Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many correct prediction we have made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532467532467533"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(test_x,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About $75.3%$ of the entire data set has been classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Optimization\n",
    "### First try\n",
    "Note that the `CountVectorizer` just split text into single words simply, we could try adjusting the parameter of how it split texts.\n",
    "\n",
    "We try set parameter `ngram_range` to `1` and `2`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vector2 = CountVectorizer(ngram_range = (1, 2))\n",
    "train_x2 = train_data['review']\n",
    "train_y2 = train_data['category']\n",
    "train_x2 = vector2.fit_transform(train_x2).toarray()\n",
    "train_x2 = np.append(train_data[['latitude', 'longitude', 'mean_checkin_time']], train_x2, axis=1)\n",
    "nb2 = GaussianNB()\n",
    "nb2.fit(train_x2, train_y2)\n",
    "\n",
    "test_x2 = test_data['review']\n",
    "test_y2 = test_data['category']\n",
    "test_x2 = vector2.transform(test_x2).toarray()\n",
    "test_x2 = np.append(test_data[['latitude', 'longitude', 'mean_checkin_time']], test_x2, axis=1)\n",
    "nb2.score(test_x2,test_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a long processing period, we got a $79.8%$ correct rate. It did an improvement. However, we should consider **wether to drop** this optimization because it cost too much time while brought not very large improvement.\n",
    "\n",
    "Because its complexity, we must comment these code in order to prevent wasting time on this optional model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second try\n",
    "Maybe we should use other Naive Bayes model after using `GaussianNB`.\n",
    "\n",
    "This is `BernoulliNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8253968253968254"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector3 = CountVectorizer()\n",
    "train_x3 = train_data['review']\n",
    "train_y3 = train_data['category']\n",
    "train_x3 = vector3.fit_transform(train_x3).toarray()\n",
    "train_x3 = np.append(train_data[['latitude', 'longitude', 'mean_checkin_time']], train_x3, axis=1)\n",
    "nb3 = BernoulliNB()\n",
    "nb3.fit(train_x3, train_y3)\n",
    "\n",
    "test_x3 = test_data['review']\n",
    "test_y3 = test_data['category']\n",
    "test_x3 = vector3.transform(test_x3).toarray()\n",
    "test_x3 = np.append(test_data[['latitude', 'longitude', 'mean_checkin_time']], test_x3, axis=1)\n",
    "nb3.score(test_x3,test_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is `MultinomialNB`.\n",
    "\n",
    "Because `MultinomialNB` cannot accept negative values, we tries to remove `latitude` and `longitude` attributes from our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8701298701298701"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector4 = CountVectorizer()\n",
    "train_x4 = train_data['review']\n",
    "train_y4 = train_data['category']\n",
    "train_x4 = vector4.fit_transform(train_x4).toarray()\n",
    "train_x4 = np.append(train_data[['mean_checkin_time']], train_x4, axis=1)\n",
    "nb4 = MultinomialNB()\n",
    "nb4.fit(train_x4, train_y4)\n",
    "\n",
    "test_x4 = test_data['review']\n",
    "test_y4 = test_data['category']\n",
    "test_x4 = vector4.transform(test_x4).toarray()\n",
    "test_x4 = np.append(test_data[['mean_checkin_time']], test_x4, axis=1)\n",
    "nb4.score(test_x4,test_y4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third try\n",
    "\n",
    "What if we combine the best 2 methods in first and second try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8051948051948052"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector5 = CountVectorizer(ngram_range = (1, 2))\n",
    "train_x5 = train_data['review']\n",
    "train_y5 = train_data['category']\n",
    "train_x5 = vector5.fit_transform(train_x5).toarray()\n",
    "train_x5 = np.append(train_data[['mean_checkin_time']], train_x5, axis=1)\n",
    "nb5 = MultinomialNB()\n",
    "nb5.fit(train_x5, train_y5)\n",
    "\n",
    "test_x5 = test_data['review']\n",
    "test_y5 = test_data['category']\n",
    "test_x5 = vector5.transform(test_x5).toarray()\n",
    "test_x5 = np.append(test_data[['mean_checkin_time']], test_x5, axis=1)\n",
    "nb5.score(test_x5,test_y5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unluckily, the combination of 2 good methods led to a worse result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "After verification, use `MultinomialNB` is a better model.\n",
    "\n",
    "Some further tries could be done. For example, we can make `latitude` and `longitude` positive. However, that didn't changed the results. So we can infer that `latitude` and `longitude` have low association with results."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
